import numpy as np
import re
import spacy

from typing import List
from math import inf

from speechless.edit_context import TimelineChange
from speechless.utils.math import ranges_of_truth

TOKEN_SEPARATOR = ' '
SENTENCE_SEPARATOR = '.' + TOKEN_SEPARATOR
SENTENCE_SEPARATOR_VALID_SET = {SENTENCE_SEPARATOR, '?' + TOKEN_SEPARATOR, '!' + TOKEN_SEPARATOR}
SENTENCE_SEPARATOR_INVALID_SET = {',' + TOKEN_SEPARATOR}

USELESS_TOKENS_SET = {'um', 'em', 'uh', 'er', 'hm', 'hmm'}

SPACY_MODEL = 'en_core_web_md'


class EditToken:
  TEXT_SUB_PATTERNS = re.compile(r'(\[[^\]]*\])|'  # text in []
                                 r'(\([^\)]*\))')  # text in ()
  WHITESPACE_SUB_PATTERNS = re.compile(r'(^\s+)|'  # whitespaces at the beginning
                                       r'(\s+$)|'  # whitespaces at the end
                                       r'((?<=\s)\s+)')  # multiple whitespaces
  WHITESPACE_PATTERN = re.compile(r'\s')

  def __init__(self, text: str, start_time: float, end_time: float):
    """Some part of the transcript, for which the timestamps are known. The text of the token is
    being normalized, which might reduce it to an empty string - this should be properly handled.
    See `TEXT_SUB_PATTERNS` and `WHITESPACE_SUB_PATTERNS` for patterns, which are removed from the
    text of the token.

    Args:
        text (str): Text that occurred in the transcript of the recording
        start_time (float): The start time of the occurrence
        end_time (float): The end time of the occurrence
    """
    self.start_time = start_time
    self.end_time = end_time
    assert self.start_time < self.end_time

    self.text = self.TEXT_SUB_PATTERNS.sub('', text)
    self.text = self.WHITESPACE_SUB_PATTERNS.sub('', self.text)
    self.text = self.WHITESPACE_PATTERN.sub(TOKEN_SEPARATOR, self.text)  # normalize whitespaces

    self.start_pos = None  # position in the document (character)
    self.index = None  # index of the token in the document
    self.label = None  # label assigned later by an analysis method

  def as_timeline_change(self, duration_ratio: float) -> TimelineChange:
    return TimelineChange(self.start_time, self.end_time, duration_ratio)

  def __len__(self) -> int:
    return len(self.text)


def spacy_nlp(text: str, pipes: List[str]) -> spacy.tokens.Doc:
  """Runs spaCy on a provided text. This function uses a static instance of spaCy language, so it is
  initialized only once.

  Args:
      text (str): Text to process
      pipes (List[str]): Which spaCy's pipes to use

  Returns:
      spacy.tokens.Doc: spaCy document generated from the text
  """
  if not hasattr(spacy_nlp, 'nlp'):  # lazy initialization of the spaCy Language
    spacy_nlp.nlp = spacy.load(SPACY_MODEL)
    spacy_nlp.nlp.Defaults.stop_words |= USELESS_TOKENS_SET | {'okay', 'right'}
  with spacy_nlp.nlp.select_pipes(enable=pipes):
    return spacy_nlp.nlp(text)


def sentence_segmentation(transcript: List[EditToken]) -> List[List[EditToken]]:
  """Segments a transcript into sentences

  Args:
      transcript (List[EditToken]): Transcript to segment

  Returns:
      List[List[EditToken]]: Segmented transcript. Each list of tokens is a separate sentence
  """
  # 0. Add a separator between tokens
  transcript[0].start_pos = 0
  for token, next_token in zip(transcript[:-1], transcript[1:]):
    token.text += TOKEN_SEPARATOR
    next_token.start_pos = token.start_pos + len(token)

  # assign indices (useful when working with sentences)
  for idx, token in enumerate(transcript):
    token.index = idx

  raw_transcript = ''.join([token.text for token in transcript])
  sentences: List[List[EditToken]] = []

  # 1. Segment using spaCy
  doc = spacy_nlp(raw_transcript, ['tok2vec', 'parser'])
  nlp_sents = list(doc.sents)
  assert nlp_sents[0].start_char == 0

  # Assign tokens to the sentences generated by spaCy
  token_idx = 0
  for sent in nlp_sents:
    sentences.append([])
    while token_idx < len(transcript):
      token = transcript[token_idx]
      if token.start_pos < sent.end_char:
        # note, that here we only check if the token starts within the current sentence, and not
        # if it ends inside it aswell. This means, that if a token extends outside of the current
        # sentence, this sentence will consume some part (or all) of the next sentence(s)
        sentences[-1].append(token)
        token_idx += 1
      else:
        break
    if len(sentences[-1]) == 0:
      # when the tokens of this sentence were already consumed by some previous sentence
      sentences.pop()

  # 2. Segment by the time between tokens
  sent_idx = 0
  while sent_idx < len(sentences):
    sent = sentences[sent_idx]
    # sent_len = (sent[-1].start_pos - sent[0].start_pos) + len(sent[-1])
    # if sent_len > 20 * 5 * 2:  # (avg_sent_length) * (avg_eng_word) * 2
    for prev_token_idx, (prev_token, token) in enumerate(zip(sent[:-1], sent[1:])):
      if (token.start_time - prev_token.end_time) > 3.0:
        token_idx = prev_token_idx + 1
        new_sent = sent[token_idx:]
        sentences[sent_idx] = sent[:token_idx]
        sentences.insert(sent_idx + 1, new_sent)
        break
    sent_idx += 1

  # 3. Add a sentence separator between sentences
  for sent in sentences[:-1]:
    sent_end = sent[-1].text
    for valid_sep in SENTENCE_SEPARATOR_VALID_SET:
      if sent_end.endswith(valid_sep):
        break
    else:
      for invalid_sep in SENTENCE_SEPARATOR_INVALID_SET:
        if sent_end.endswith(invalid_sep):
          sent_end = sent_end[:-len(invalid_sep)] + SENTENCE_SEPARATOR
          break
      else:
        sent_end = sent_end[:-len(TOKEN_SEPARATOR)] + SENTENCE_SEPARATOR
      sent[-1].text = sent_end

  # 4. Fix starting positions of tokens
  tokens = [token for sentence in sentences for token in sentence]
  for token, next_token in zip(tokens[:-1], tokens[1:]):
    next_token.start_pos = token.start_pos + len(token)

  return sentences


def make_timeline_changes(tokens: List[EditToken],
                          duration_ratio: float = 0.0) -> List[TimelineChange]:
  """Create list of timeline changes ready for modification

  Args:
      tokens (List[EditToken]): List of labeled tokens
      duration_ratio (float): Duration ratio for the timeline changes

  Returns:
      List[TimelineChanges]: List of changes, with intervals between words
      and before first and after last word
  """
  rot = ranges_of_truth(np.array([not token.label for token in tokens]))
  changes = []
  for r in rot:
    start = tokens[r[0] - 1].end_time if r[0] > 0 else 0.0
    end = tokens[r[1]].start_time if r[1] < len(tokens) else inf
    changes.append(TimelineChange(start, end, duration_ratio))
  if len(changes) == 0:
    return changes
  if changes[0].beg != 0.0:
    changes.insert(0, TimelineChange(0.0, tokens[0].start_time, duration_ratio))
  if changes[-1].end != inf:
    changes.append(TimelineChange(tokens[-1].end_time, inf, duration_ratio))
  return changes
